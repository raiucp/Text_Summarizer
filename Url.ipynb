{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"1:\")\n",
    "import nltk\n",
    "import string\n",
    "from lxml import html\n",
    "import requests\n",
    "from readability import Document\n",
    "from urllib import request\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2\n",
    "response = requests.get(\"https://www.ndtv.com/mumbai-news/rain-causes-wall-collapse-in-mumbais-wadala-cars-stuck-in-debris-1872626\")\n",
    "tree = html.fromstring(response.content)\n",
    "doc = Document(response.text)\n",
    "# print(doc.title())\n",
    "tree_text = tree.xpath('//p/text() | //p/a/text() |//p/b/text() | //div/text() | //h2/text() | //h1/text() | //h2/a/text() | //h3/text() | //h3/a/text()')\n",
    "# tree_text = tree.xpath('//div/text()')\n",
    "# print(tree_text)\n",
    "# response=request.urlopen(\"https://en.wikipedia.org/wiki/Louis_Tomlinson\")\n",
    "# print(\"URL:\",response.geturl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"2:\")\n",
    "text=\"\"\n",
    "file=open('text_with_b.txt','w+')\n",
    "for x in tree_text:\n",
    "    if(\"'b'\" in x):\n",
    "        x.strip(\"'b'\")\n",
    "    file.write(str(x.encode(\"utf-8\")))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"3:\")\n",
    "s0='\"b\"'\n",
    "s1=\"'b'\"\n",
    "s2=\"'b\"\n",
    "s3=\"b'\"\n",
    "# print(set(string.printable))\n",
    "# pattern = \"\\\"(?=[<\\\"]+,)[>\\\"]+\\\"\"\n",
    "with open('text_with_b.txt', 'r') as infile, \\\n",
    "     open('text_document.txt', 'w') as outfile:\n",
    "    data = infile.read()\n",
    "    data = data.replace(s0,\"\")\n",
    "    data = data.replace(s1, \"\")\n",
    "    data = data.replace(s2, \"\")\n",
    "    data = data.replace(s3, \"\")\n",
    "    outfile.write(data)\n",
    "infile.close()\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"3:\")\n",
    "# sent=[]\n",
    "# with open('text_document.txt', 'r') as file:\n",
    "#     sent=sent_tokenize(file.read())\n",
    "# # print(sent)\n",
    "# # print(len(sent))\n",
    "# tokens=[]\n",
    "# for i in range(0,len(sent)):\n",
    "#     tokens.append(word_tokenize(sent[i]))\n",
    "#     i=i+1\n",
    "# # print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting summarizer \n",
    "# print(\"4:\")\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import copy\n",
    "\n",
    "class FrequencySummarizer:\n",
    "  def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "    \"\"\"\n",
    "     Initilize the text summarizer.\n",
    "     Words that have a frequency term lower than min_cut \n",
    "     or higer than max_cut will be ignored.\n",
    "    \"\"\"\n",
    "    self._min_cut = min_cut\n",
    "    self._max_cut = max_cut \n",
    "    self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "  def _compute_frequencies(self, word_sent):\n",
    "    \"\"\" \n",
    "      Compute the frequency of each of word.\n",
    "      Input: \n",
    "       word_sent, a list of sentences already tokenized.\n",
    "      Output: \n",
    "       freq, a dictionary where freq[w] is the frequency of w.\n",
    "    \"\"\"\n",
    "    freq = defaultdict(int)\n",
    "    for s in word_sent:\n",
    "      for word in s:\n",
    "        if word not in self._stopwords:\n",
    "          freq[word] += 1\n",
    "    # frequencies normalization and fitering\n",
    "#     print(freq)\n",
    "    m = float(max(freq.values()))\n",
    "    freq_words = copy.deepcopy(freq)\n",
    "    for w in freq.keys():\n",
    "      freq[w] = freq[w]/m\n",
    "      freq_words[w] = freq_words[w]/m\n",
    "      if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "        del freq_words[w]\n",
    "    return freq_words\n",
    "\n",
    "  def summarize(self, text, n):\n",
    "    \"\"\"\n",
    "      Return a list of n sentences \n",
    "      which represent the summary of text.\n",
    "    \"\"\"\n",
    "    sents = sent_tokenize(text)\n",
    "    assert n <= len(sents)\n",
    "    word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "    self._freq = self._compute_frequencies(word_sent)\n",
    "    ranking = defaultdict(int)\n",
    "    for i,sent in enumerate(word_sent):\n",
    "      for w in sent:\n",
    "        if w in self._freq:\n",
    "          ranking[i] += self._freq[w]\n",
    "    sents_idx = self._rank(ranking, n)    \n",
    "    return [sents[j] for j in sents_idx]\n",
    "\n",
    "  def _rank(self, ranking, n):\n",
    "    \"\"\" return the first n sentences with highest ranking \"\"\"\n",
    "    return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mumbai Rains: Wall Collapses In Wadala East, Cars Stuck In Debris\n",
      "\n",
      "* We are waiting for the structural engineer's clearance just to be safe, he said.\\xc2\\xa0\"\"It has been raining continuously for the last few days and around 4 this morning, there was a deafening sound and the boundary wall of our complex came crashing.\n",
      "\n",
      "* It was almost like a landslide and 15-20 cars went with it,\" a resident described  | Edited by  |  \\xc2\\xa0The wall collapsed around 4 am and at least 20 vehicles went down after the road caved in.Highlights\\r\\n                                A portion of a wall at an apartment complex in south Mumbai collapsed this morning following \", which led to a road caving in and taking down at least 20 parked cars.\n",
      "\n",
      "* It was almost like a landslide and 15-20 cars went with it,\" he described.\\xc2\\xa0An entire wing of the building has been evacuated.Residents claim that construction activity in the neighbourhood had weakened the wall.Construction work next door reportedly weakened the boundary wall of the parking area, which gave away in the rain, residents said.\n",
      "\n",
      "* The wall collapsed this morning after heavy rain,\" said Anila Pillai, a consultant, who also lives in the complex.\\xc2\\xa0Mumbai received extremely heavy rainfall in the last 24 hours.Major landslide at #Wadala.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"5:\")\n",
    "print(doc.title())\n",
    "print()\n",
    "fs = FrequencySummarizer()\n",
    "with open(\"text_document.txt\",\"r\") as file:\n",
    "    text=file.read()\n",
    "    for s in fs.summarize(text,4):\n",
    "        print('*',s)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
